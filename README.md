
# Effortlessly Compare Codes Generated by LLMs: Mistral, Codellama & Wizardcoder using Ollama

The session focuses on how to use Ollama (https://ollama.ai/) to run open-source models in local machines such as Mac & Linux. The same can be used in Windows too using WSL2. 

The detailed session/video can be found [here](https://youtu.be/wumUxmQAjH8).

The json file is generated using Google Bard which is included in this repo.

P.S. There can be significant difference in performance when the LLMs are used in Perplexity or other playgrounds or GPU based setups to when they are used in local machines owing to various factors. This video/session in no way undermines the performance of any model. It is just an attempt to understand how Ollama can be used to run  various LLMs in local machines.

Lot factors such as model size, availability of GPU, token size etc will impact the performance of the models.

## Ollama

The documentation & steps to download Ollama is here:
[Ollama](https://ollama.ai/)



### Mistral

Mistral 7b v0.1 is Mistral AI's first LLM which   consistently outperforms Llama2-13B on all metrics and is competitive with Llama 34B.

Mistral official documentation can be found here:
[Mistral](https://docs.mistral.ai/)

Mistral usage through Ollama link below:
[Mistral using Ollama](https://ollama.ai/library/mistral)


### Codellama

[Code Llama](https://github.com/facebookresearch/codellama) is a family of large language models for with flavors in code generation & instruction-following models which is consistently rated among the top LLMs and is based on Llama 2 by Facebook research.

Codellama usage through Ollama link is given [here](https://ollama.ai/library/codellama)

### Wizard coder

wizardcoder usage through Ollama link is given [here](https://ollama.ai/library/wizardcoder)



